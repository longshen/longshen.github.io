<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Project_management | longshen]]></title>
  <link href="http://longshen.github.io/blog/categories/project-management/atom.xml" rel="self"/>
  <link href="http://longshen.github.io/"/>
  <updated>2016-04-25T21:23:42+08:00</updated>
  <id>http://longshen.github.io/</id>
  <author>
    <name><![CDATA[longshen]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据驱动业务_经验总结]]></title>
    <link href="http://longshen.github.io/blog/2015/11/28/data-driven-business/"/>
    <updated>2015-11-28T16:59:58+08:00</updated>
    <id>http://longshen.github.io/blog/2015/11/28/data-driven-business</id>
    <content type="html"><![CDATA[<blockquote><p>从2013年开始负责项目就开始尝试让数据驱动业务，但对于什么是数据驱动，如何用数据驱动业务都非常模糊。经过最近一年实践，对数据驱动业务有进一步的认识。对于专业的数据平台团队而言，这篇文章可能只是一个门外汉的一些见解；而对于非数据平台的业务团队，也许有一些可参考的。</p></blockquote>

<h2>一、什么是数据驱动</h2>

<p><strong>简单说就是拿数据说话，在产品开发的各个环节使用数据做决策。</strong>在产品设计初期，通过调研、问卷等获得用户对产品的需求，进而驱动产品的设计。在产品运营阶段，通过A/B测试，数据对比，优化产品等等。</p>

<p>后续部分，主要从以下这几个方面介绍我们团队在数据驱动业务方面的实践。
<img src="/media/14487012662534.jpg" alt="" /></p>

<h2>二、基础工具</h2>

<h3>1、数据平台</h3>

<p>数据平台包括数据接收服务、数据仓库（hadoop）、内存计算（hive、Spark）、调度（资源调度、程序调度）等。<strong>它实现支持数据采集、存储、计算这些重要的环节，是数据驱动的基础</strong>。</p>

<h3>2、数据可视化</h3>

<p>将数据库中每一个数据项作为单个图元元素表示，构成大量数据集的图像。常见有折线图、柱状图、饼图。
目前比较流行的有iCharts、d3.js等，而我们团队数据可视化只需要支持一些基础图形便能满足需求，我们选择一个轻巧的开源库<a href="http://www.ichartjs.com/">ichartjs</a>。</p>

<h3>3、监控预警</h3>

<p><strong>数据可视化的工具基本上都是需要主动去查阅的</strong>，当数据分析维度变多，个别维度的数据异常将容易被忽视或者无法被及时发现。<strong>可以加入一些监控预警机制，在预定条件被触发时，被动接收预警</strong>，保证及时发现。比如达到某一阀值时进行邮件预警，严重的进行电话预警。</p>

<h3>4、简化统计脚本</h3>

<p>简化统计脚本对于专业的数据分析人员重要，对于业务人员也非常重要。它使业务人员分析数据门槛降低，让业务人员分析数据成为可能。而业务人员对数据的分析，虽然分析方法上不如专业的数据分析人员，但是<strong>依靠业务人员对业务的深入理解，在问题深入分析定位方面往往比数据分析人员更容易挖掘原因和优化点</strong>。
可以通过对原始数据库进行预处理（分表串表等），提供常用函数（格式转换、分区域、分运营等）等方式，简化新增、修改统计脚本。</p>

<h2>三、数据收集</h2>

<p>即用户使用产品时触发一些统计埋点，生成统计数据，上报到数据平台。不同公司、不同业务，统计埋点、统计上报方式、协议都不相同，以下主要介绍几个重要的细节点。</p>

<h3>1、尽量减少需要依赖、统计时需要串联的统计埋点</h3>

<p>什么是需要依赖、统计时需要串联的统计埋点，比如一次下载过程性能情况的统计，可以简单划分为4个环节，该次下载发起的连接次数、连接成功次数、请求次数、接收数据的次数。统计埋点可以在每个统计环节都做埋点然后各自上报。当要分析时需要把这4个环节的数据进行串联或者统计汇总，才能确定性能情况。
尽量减少需要依赖、统计时需要串联的统计埋点，有以下量点好处</p>

<ul>
<li><p>减少上报数据丢失的影响</p>

<blockquote><p>aa一般项目的数据上报流程无法做到100%数据都上报到统计平台。如果能保证95%有效数据达到，对于需要有4个数据记录进行串联的统计，能使用的有效数据就不到82%。而如果在客户端汇总并指上报一条记录，则能保证有效数据还是95%。</p></blockquote></li>
<li><p>降低统计分析的门槛</p>

<blockquote><p>数据串联建表本身耗时并且繁琐的事情，减少这些事情统计脚本可以精简很多</p></blockquote></li>
</ul>


<h3>2、完善的A/B测试系统</h3>

<p>这个主要涉及功能点数据验证难点及数据收集周期的问题。有A/B测试，能在验证功能的时候，保证尽量少影响用户（同时也增加了验证功能点的可能性）。而完善的A/B测试系统，可以支持同时灰度多种策略/方案，减少数据收集的周期。</p>

<h3>3、任何需求，数据优先</h3>

<p>每一个需求开始设计/实现时，则考虑以下两个问题，提前考虑好需要的埋点。
<code>
1、如何验证需求效果，如何统计（如何写统计脚本），需要哪些字段
2、如果效果不理想，如何定位问题（如何写分析脚本），需要哪些字段
</code>
需求测试时，做好以下两个事情，在版本发布前保证好统计埋点正确性。
<code>
1、验证统计埋点数据每个字段的统计时机、字段值是否正确
2、触发上报一批统计数据，使用已写好的脚本进行统计，确认统计结果是否与预期一致
</code></p>

<blockquote><p>通常的需求，统计埋点的工作量相对较少。但由于对统计埋点的重视度不够，往往等到发版上线后才进行统计。这时发现缺少某些统计埋点，然后等待下个版本添加。有时甚至需要几个版本迭代才能完成需要的统计埋点。导致那些需要依赖数据进行选择、优化的需求，浪费了几个版本的迭代优化时间。</p></blockquote>

<h2>四、数据分析</h2>

<p>通常的分析步骤如下：
1、确定目标（问题），明确要分析目标或者问题
2、分解，从不同维度去分析
3、评估，根据分析的数据情况，评估某个维度的影响
4、决策，从评估结果决策下一步分解的内容，或者得出结论。
5、不断循环细化分解，执行2~4步，直到得出结论（可能多个结论）。
在2~4步骤中，可以尝试用自问自答的方式，进行分解、评估。<strong>可以借助思维导图工具（Xmind、MindManger）去发散问题，记录分解的维度和评估决策情况。</strong></p>

<blockquote><p>常用的分析方法有对比、聚类、结构、因子、交叉等。
比如在分析错误原因时，更多的使用结构分析方法，列出业务各个流程导致错误的占比，重点分析优化高占比部分。</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[文件同步工具_Seafile]]></title>
    <link href="http://longshen.github.io/blog/2015/11/16/file-syn-seafile/"/>
    <updated>2015-11-16T22:49:10+08:00</updated>
    <id>http://longshen.github.io/blog/2015/11/16/file-syn-seafile</id>
    <content type="html"><![CDATA[<blockquote><p>项目开发过程，团队成员经常协同编辑同一个文件。普通文档类的，可以使用自建的Confluence、Wordpress等，或者在线服务Worktitle、tower等。而<strong>像excel、思维导图（xmind）等复杂的文档文件</strong>，这些在线编辑协作的工具都未能支持，需要靠同步文件进行协作。</p>

<p>可惜 Dropbox 在国内多次遭屏蔽，而国产的百度云、快盘等产品也不太稳定。为保证协作效率，可以使用Seafile自建云存储平台。</p></blockquote>

<h2>服务器</h2>

<p>直接见<a href="http://www.centoscn.com/CentosServer/ftp/2015/0324/5003.html">Centos部署Seafile开源文件云存储服务器</a></p>

<blockquote><p>记得关闭防火墙</p>

<p><code>/sbin/iptables -I INPUT -p tcp --dport 8000 -j ACCEPT</code>
<code>/sbin/iptables -I INPUT -p tcp --dport 8082 -j ACCEPT</code></p></blockquote>

<h2>客户端</h2>

<p><a href="https://www.seafile.com/download/">Seafile官网下载链接</a></p>

<h2>其他问题</h2>

<h3>服务器宕机，数据如何恢复</h3>

<p>Seafile分两类数据，一类是数据库（用户信息等），一类是资料库数据（用户共享的文件库）。</p>

<p><strong>不备份的影响</strong>
第一类在SeaHub宕机（当然指数据无法恢复那种）后，则丢失了。
第二类有点像git，在客户端会有副本。在SeaHub重新启动后，可以从客户端重新提交。（具体丢失的数据量取决于客户端副本的更新时间）</p>

<p><strong>备份方法</strong>
1、备份数据
（不同路径、mysql账号密码、备份机器IP记得修改）</p>

<pre><code class="bash">#!/bin/bash
backup_dir="/usr/local/file/backup_db"
mysqldump_cmd="mysqldump -uroot -p123456 --opt"

$mysqldump_cmd ccnet-db &gt; $backup_dir/ccnet-db.sql.`date +"%Y-%m-%d-%H-%M-%S"`
$mysqldump_cmd seafile-db &gt; $backup_dir/seafile-db.sql.`date +"%Y-%m-%d-%H-%M-%S"`
$mysqldump_cmd seahub-db &gt; $backup_dir/seahub-db.sql.`date +"%Y-%m-%d-%H-%M-%S"`
echo "backup db success!"
rsync -va . root@10.10.159.176:/usr/local/file/
echo "sync data to 10.10.159.176 success!"
</code></pre>

<p>2、定时备份==
使用crontab定时执行第1步脚本。</p>

<pre><code class="bash">crontab -e
#添加如下执行任务（每天0点0分进行备份）
0 0 * * * /usr/local/file/backup_seafile_data.sh

#重新加载crontab配置
/sbin/service crond reload

#Note: 需要为脚本添加可执行权限，否则定时任务不生效
chmod +x /usr/local/file/backup_seafile_data.sh
</code></pre>

<p>rsyc方法参考<a href="http://longshen.github.io/blog/2015/11/18/sync-file-between-linux/">Linux间传输文件</a>
具体备份内容细见<a href="http://manual-cn.seafile.com/maintain/backup_recovery.html">Seafile备份与恢复</a></p>

<h2>参考</h2>

<p><a href="http://manual-cn.seafile.com/index.html">Seafile服务器中文手册</a></p>
]]></content>
  </entry>
  
</feed>
